Good scalability It is designed to work with large volumes of documents, and tested with several million
documents. The current implementation would require further work to scale to billions of documents
(e.g.: process some data structures on disk instead of in memory).
Highly configurable All of the parameters for crawling and indexing can be configured, including several
scheduling policies.
High performance It is entirely written in C/C++ for high performance. The downloader modules of the
WIRE crawler (“harvesters”) can be executed in several machines.
Open-source The programs and the code are freely available under the GPL license.


Acquaintances, friends and social popularity in human interactions. The Economist commented “in
other words, some people have all the luck, while others have none.” [Eco02].
• Sexual partners in humans, which is highly relevant for the control of sexually-transmitted diseases.
• Power grid designs, as most of them are designed in such a way that if a few key nodes fail, the entire
system goes down.
• Collaboration of movie actors in films.
• Citations in scientific publications.
• Protein interactions in cellular metabolism.

Examples of scale-free networks related to the Internet are:
• Geographic, physical connectivity of Internet nodes.
• Number of links on Web pages.
• User participation in interest groups and communities.
• E-mail exchanges.


Manager: page value calculations and long-term scheduling.
Harvester: short-term scheduling and network transfers.
Gatherer: parsing and link extraction.
Seeder: URL resolving and link structure.


Our link structure does not use compression. The Web graph can be compressed by exploiting the
locality of the links, the distribution of the degree of pages, and the fact that several pages share a substantial



We argue that the number of pages on the Web can be considered infinite, and given that a Web crawler
cannot download all the pages, it is important to capture the most important ones as early as possible during
the crawling process. We propose, study, and implement algorithms for achieving this goal, showing that we
can crawl 50% of a large Web collection and capture 80% of its total Pagerank value in both simulated and
real Web environments


69 
126



A SURVEY OF FOCUSED WEB CRAWLING ALGORITHMS
Blaz Novak, 2004
==================================================================================
PASCAL - Pattern Analysis, Statistical Modelling and Computational Learning


Focused Crawlers

Improving the performance of focused web crawlers
Sotiris Batsakisa, , , Euripides G.M. Petrakisa, , Evangelos Miliosb 
==================================================================================

This work addresses issues related to the design and implementation of focused crawlers. Several variants of state-of-the-art crawlers relying on web page content and link information for estimating the relevance of web pages to a given topic are proposed. Particular emphasis is given to crawlers capable of learning not only the content of relevant pages (as classic crawlers do) but also paths leading to relevant pages. A novel learning crawler inspired by a previously proposed Hidden Markov Model (HMM) crawler is described as well. The crawlers have been implemented using the same baseline implementation (only the priority assignment function differs in each crawler) providing an unbiased evaluation framework for a comparative analysis of their performance. All crawlers achieve their maximum performance when a combination of web page content and (link) anchor text is used for assigning download priorities to web pages. Furthermore, the new HMM crawler improved the performance of the original HMM crawler and also outperforms classic focused crawlers in searching for specialized topics.


Focused crawling: a new approach to topic-specific Web resource discovery
Soumen Chakrabartia, , 1, Martin van den Bergb, 2, Byron Domc
==================================================================================
(...) 
To achieve such goal-directed crawling, we designed two hypertext mining programs that guide our crawler: a classifier that evaluates the relevance of a hypertext document with respect to the focus topics, and a distiller that identifies hypertext nodes that are great access points to many relevant pages within a few links. We report on extensive focused-crawling experiments using several topics at different levels of specificity. Focused crawling acquires relevant pages steadily while standard crawling quickly loses its way, even though they are started from the same root set. Focused crawling is robust against large perturbations in the starting set of URLs. It discovers largely overlapping sets of resources in spite of these perturbations. It is also capable of exploring out and discovering valuable resources that are dozens of links away from the start set, while carefully pruning the millions of pages that may lie within this same radius. Our anecdotes suggest that focused crawling is very effective for building high-quality collections of Web documents on specific topics, using modest desktop hardware.


****************************************
General Crawlers
****************************************

Grub 
====
C++ . Wikia Search. Good alternative


HTTrack
=======
Written in C. Mirroring.
There is a version for windows. Cool video.
http://www.httrack.com/page/1/en/index.html

Dig Group
========================================
Open-source, but last update: 2006
http://www.htdig.org/

Nutch
========================================
Recently updated. Giant. Java.
http://nutch.apache.org/

Scrapy
========================================
Python

Heritrix
========================================
Internet Archive. Java

cURL x wget
========================================
1) "In summary, curl is much more robust, but only wget supports recursive downloads. –  Sean Aug 26 at 16:43
add a comment"

2) "If you are programming, you should use curl. It has a nice api and is available for most languages. Shelling out to the os to run wget is a kludge and shouldn't be done if you have an API interface!"

DataparkSearch
========================================
Search Engine. So much powerfull.
http://www.dataparksearch.org/


ICDL Crawling
========================================
Website Parse Template
Dont understand. Japanese.
"WPT is an XML based open format which provides HTML structure description of website pages. WPT format allows web crawlers to generate Semantic Web’s RDF triplets for web pages. WPT is compatible with existing Semantic Web concepts defined by W3C (RDF and OWL) and UNL specifications."








