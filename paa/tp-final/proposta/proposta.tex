\documentclass[a4paper,12pt,titlepage]{article}
%\usepackage[T1]{fontenc}

\usepackage{titlesec}
\usepackage{titling}
\usepackage[portuguese]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{indentfirst}
\usepackage{graphicx}
%\usepackage{times}
\usepackage{ucs}
\usepackage{float}    
\usepackage{fancyvrb}   
\usepackage{verbatim}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{epigraph}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{lipsum}

\hypersetup{
    colorlinks=true,       
    linkcolor=black,          
    citecolor=black,   
    filecolor=black,  
    urlcolor=black  
}

\hyphenation {di-re-cio-na-men-to} 

%\renewcommand*{\familydefault}{\ttdefault}
\lstset{columns=fullflexible,basicstyle=\ttfamily}

\title{\large
Universidade Federal de Minas Gerais \\ \
Instituto de Ciências Exatas \\ \ 
Departamento de Ciência da Computação \\ \
\\[1cm]
Projeto de Final de Curso\\ \
Projeto e Análise de Algoritmos\\ \
\\[1cm]
\textbf{\Large Proposta de Trabalho }
\\[1cm]
}

\author{\large Alberto Hideki Ueda \\[0.5cm] 
	Orientador: Berthier Ribeiro Neto 
\\[3cm] }

\date{\textsc{Belo Horizonte\\ \
6 de outubro de 2014}}

\begin{document}
\maketitle

\pagebreak

% Qual o problema?
Coletores de páginas da \textit{Web} constituem o primeiro passo para a implementação de máquinas de busca modernas. De forma geral, um coletor - em inglês, \textit{crawler} - é um sistema que faz requisições a servidores da \textit{Web} de forma planejada e automática, coleta parte ou todo o conteúdo das páginas devolvidas pelas requisições e utiliza este novo conteúdo para realizar novas requisições \cite{b}. Estima-se que hoje mais de 10\% das visitas a \textit{Websites} sejam feitas por coletores \cite{nielsen}.

O primeiro coletor \textit{Web} conhecido foi criado em 1993 por Matthew Gray - então graduando do MIT - e chamava-se WWWW (\textit{World Wide Web Wanderer)}. Comprovando a forte relação com a história dos sistemas de busca na \textit{Web}, no mesmo ano foi lançada também a primeira máquina de busca conhecida, ALIWEB, criada por Martijn Koster \cite{b}. Nesta época, um número razoável de servidores para se obter uma boa cobertura da rede girava em torno de apenas alguns milhares. Desde então, o número de \textit{hosts} tem aumentado em alta velocidade  (chegando a praticamente dobrar a cada ano, de 1993 a 1996 \cite{gray}), tornando as máquinas de busca ainda mais necessárias e, consequentemente, também os coletores de dados.

% Por que ele é um problema difícil?
Hoje, porém, mesmo as principais máquinas de busca cobrem apenas uma parte da \textit{Web} atual. Em 2005, foi demonstrado que o nível de cobertura das principais máquinas de buscas existentes está entre 58\% e 76\% da \textit{Web} \cite{gulli}. Além disso, o custo da utilização da rede também deve ser considerado. Em 2004, tal custo foi estimado em US\$ 1,5 milhão para coletores de larga escala \cite{craswell}. 

Portanto, tal problema pode ser considerado de difícil resolução, dado o tamanho da entrada necessária para uma solução exata. A ideia deste trabalho é modificar um algoritmo existente de um coletor genérico (\textit{General Crawler}) e transformá-lo em um coletor temático (\textit{Focused Crawler}) - que concentra-se em um único tópico de interesse - visando aumentar tanto a qualidade quanto a cobertura das páginas coletadas em relação ao algoritmo original.

Mais especificamente, este trabalho consistirá em alterações no escalonamento de longo prazo de um \textit{General Crawler}, direcionando as requisições de \textit{downloads} para páginas relevantes a um tópico pré-estabelecido, por meio de consultas de referência (\textit{driving queries}) ou documentos de exemplo (como um conjunto de páginas \textit{Web}). As páginas da \textit{Web} consideradas para este trabalho serão tanto públicas (não são protegidas por autenticação de usuário) quanto estáticas (não são criadas dinamicamente pela entrada do usuário). Considerando as páginas da \textit{Web} como vértices e os diversos \textit{links} encontrados como arestas, pode-se aplicar diferentes algoritmos para a seleção \textit{on-line} (em tempo de execução) dos \textit{links} que o coletor irá visitar em um nova coleta, destacando-se estratégias \textit{Breadh-first} \cite{najork}, utilização de \textit{PageRank} \cite{cho} e até mesmo o uso de algoritmos genéticos \cite{johnson}.

% Referências
\bibliographystyle{plain}%amsalpha
\bibliography{bibliografia}
\newpage

\end{document}


