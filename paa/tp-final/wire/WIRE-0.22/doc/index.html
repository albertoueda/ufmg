<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>WIRE Documentation</title><meta name="generator" content="DocBook XSL Stylesheets V1.73.2"><meta name="description" content="This document describes the WIRE application, including instruction on how to run WIRE, and the API for programming extensions to WIRE."></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="book" lang="en"><div class="titlepage"><div><div><h1 class="title"><a name="wire"></a>WIRE Documentation</h1></div><div><div class="author"><h3 class="author"><span class="firstname">Carlos</span> <span class="othername">C.</span> <span class="surname">Castillo</span></h3><div class="affiliation"><span class="orgname">Center for Web Research<br></span><div class="address"><p><br>
          <code class="email">&lt;<a class="email" href="mailto:ccastill@dcc.uchile.cl">ccastill@dcc.uchile.cl</a>&gt;</code><br>
        </p></div></div></div></div><div><div class="abstract"><p class="title"><b>Abstract</b></p><p>
			This document describes the WIRE application, including instruction
			on how to run WIRE, and the API for programming extensions to WIRE.
		</p></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#overview">1. Overview</a></span></dt><dt><span class="chapter"><a href="#installation">2. Installation</a></span></dt><dd><dl><dt><span class="section"><a href="#installation-download">Downloading</a></span></dt><dt><span class="section"><a href="#installation-requirements">Requirements</a></span></dt><dt><span class="section"><a href="#installation-overview">Installing</a></span></dt><dt><span class="section"><a href="#installation-cvs">If you have access to the CVS</a></span></dt><dt><span class="section"><a href="#installation-tunning">Kernel tunning</a></span></dt></dl></dd><dt><span class="chapter"><a href="#configuration">3. Configuration</a></span></dt><dd><dl><dt><span class="section"><a href="#configuration-overview">Overview</a></span></dt><dt><span class="section"><a href="#configuration-example">Sample configuration</a></span></dt></dl></dd><dt><span class="chapter"><a href="#repository">4. Data Format and API for the Repository</a></span></dt><dd><dl><dt><span class="section"><a href="#repository-overview">Overview</a></span></dt><dt><span class="section"><a href="#repository-storage">Text and HTML storage (text/)</a></span></dt><dd><dl><dt><span class="section"><a href="#repository-storage-lfs">Large file support</a></span></dt><dt><span class="section"><a href="#repository-storage-api">API</a></span></dt></dl></dd><dt><span class="section"><a href="#repository-metaidx">Metadata index (metaidx/)</a></span></dt><dd><dl><dt><span class="section"><a href="#repository-metaidx-api">API</a></span></dt></dl></dd><dt><span class="section"><a href="#repository-urlidx">URL index (urlidx/)</a></span></dt><dd><dl><dt><span class="section"><a href="#repository-urlidx-api">API</a></span></dt></dl></dd><dt><span class="section"><a href="#repository-linkidx">Link index (link/)</a></span></dt><dt><span class="section"><a href="#repository-harvestidx">Harvest index (harvest/)</a></span></dt></dl></dd><dt><span class="chapter"><a href="#bot">5. Web crawler</a></span></dt><dd><dl><dt><span class="section"><a href="#bot-reset">wire-bot-reset</a></span></dt><dt><span class="section"><a href="#bot-seeder">wire-bot-seeder</a></span></dt><dt><span class="section"><a href="#bot-manager">wire-bot-manager</a></span></dt><dd><dl><dt><span class="section"><a href="#bot-manager-score">Score function</a></span></dt></dl></dd><dt><span class="section"><a href="#bot-harvester">wire-bot-harvester</a></span></dt><dt><span class="section"><a href="#bot-gatherer">wire-bot-gatherer</a></span></dt><dt><span class="section"><a href="#bot-run">wire-bot-run</a></span></dt><dt><span class="section"><a href="#bot-howto">Step-by-step instructions for running the crawler</a></span></dt><dd><dl><dt><span class="section"><a href="#bot-howto-1">1. Create the directory for the collection</a></span></dt><dt><span class="section"><a href="#bot-howto-2">2. Copy the sample configuration file into the directory</a></span></dt><dt><span class="section"><a href="#bot-howto-3">3. Set the environment variable <code class="envar">WIRE_CONF</code></a></span></dt><dt><span class="section"><a href="#bot-howto-4">4. Edit the configuration file, mandatory parameters</a></span></dt><dt><span class="section"><a href="#bot-howto-5">5. Create a blank collection</a></span></dt><dt><span class="section"><a href="#bot-howto-6">6. Add starting URLS</a></span></dt><dt><span class="section"><a href="#bot-howto-7">7. Run TWO test crawler cycles</a></span></dt><dt><span class="section"><a href="#bot-howto-8">8. Run several crawler cycles</a></span></dt></dl></dd><dt><span class="section"><a href="#run-stop">Stopping the crawler</a></span></dt></dl></dd><dt><span class="chapter"><a href="#info">6. Statistics and reports</a></span></dt><dd><dl><dt><span class="section"><a href="#info-shell">Interactive querying</a></span></dt><dd><dl><dt><span class="section"><a href="#info-shell-main">Main commands</a></span></dt><dt><span class="section"><a href="#info-shell-doc">Information about a document</a></span></dt><dt><span class="section"><a href="#info-shell-site">Information about a site</a></span></dt><dt><span class="section"><a href="#info-shell-harvester">Information about a harvester</a></span></dt><dt><span class="section"><a href="#info-shell-repository">Information about the collection</a></span></dt></dl></dd><dt><span class="section"><a href="#info-extract">Extract raw data</a></span></dt><dt><span class="section"><a href="#info-analysis">Analyzing data to generate statistics</a></span></dt><dd><dl><dt><span class="section"><a href="#info-analysis-link">Link analysis in the graph of pages</a></span></dt><dt><span class="section"><a href="#info-analysis-sitelink">Link analysis in the graph of websites</a></span></dt><dt><span class="section"><a href="#info-analysis-stats">Generating statistics</a></span></dt></dl></dd><dt><span class="section"><a href="#info-report">Generating reports</a></span></dt><dd><dl><dt><span class="section"><a href="#info-report-conf">Configuring the reports</a></span></dt><dt><span class="section"><a href="#info-report-procedure">Procedure for generating reports</a></span></dt><dt><span class="section"><a href="#info-report-doc">Report about documents</a></span></dt><dt><span class="section"><a href="#info-report-site">Report about sites</a></span></dt><dt><span class="section"><a href="#info-report-link">Report about the graph of links between sites</a></span></dt><dt><span class="section"><a href="#info-report-harvest">Report about the harvest batches</a></span></dt><dt><span class="section"><a href="#info-report-lang">Report about document languages</a></span></dt><dt><span class="section"><a href="#info-report-extension">Report about the link extensions</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#index">7. Indexing and searching</a></span></dt><dd><dl><dt><span class="section"><a href="#index-create">Creating the index</a></span></dt><dt><span class="section"><a href="#index-use">Using the index: normal search</a></span></dt></dl></dd><dt><span class="chapter"><a href="#history">8. Version history</a></span></dt><dd><dl><dt><span class="section"><a href="#id2791072">Version 0.22</a></span></dt><dt><span class="section"><a href="#id2791086">Version 0.21</a></span></dt><dt><span class="section"><a href="#id2791139">Version 0.20</a></span></dt><dt><span class="section"><a href="#id2791153">Version 0.19</a></span></dt><dt><span class="section"><a href="#id2791174">Version 0.18</a></span></dt><dt><span class="section"><a href="#id2791190">Version 0.17</a></span></dt><dt><span class="section"><a href="#id2791204">Version 0.16</a></span></dt><dt><span class="section"><a href="#id2791225">Version 0.15</a></span></dt><dt><span class="section"><a href="#id2791246">Version 0.14</a></span></dt><dt><span class="section"><a href="#id2791260">Version 0.13</a></span></dt><dt><span class="section"><a href="#id2791296">Version 0.12</a></span></dt><dt><span class="section"><a href="#id2791329">Version 0.11</a></span></dt><dt><span class="section"><a href="#id2791365">Version 0.10</a></span></dt><dt><span class="section"><a href="#id2791388">Version 0.9b</a></span></dt><dt><span class="section"><a href="#id2791402">Version 0.9</a></span></dt><dt><span class="section"><a href="#id2791425">Version 0.8</a></span></dt></dl></dd><dt><span class="chapter"><a href="#bugs">9. Known bugs</a></span></dt><dt><span class="chapter"><a href="#acknowledgements">10. Acknowledgements</a></span></dt></dl></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="overview"></a>Chapter 1. Overview</h2></div></div></div><p>The WIRE project is an effort started by the <a class="ulink" href="http://www.cwr.cl/" target="_top">Center for Web Research</a> for creating an application for information retrieval, designed to be used on the Web.</p><p>Currently, it includes:</p><div class="itemizedlist"><ul type="disc"><li><p>A simple format for storing a collection of web documents.</p></li><li><p>A web crawler.</p></li><li><p>Tools for extracting statistics from the collection.</p></li><li><p>Tools for generating reports about the collection.</p></li></ul></div><p>The main characteristics of the WIRE software are:</p><div class="itemizedlist"><ul type="disc"><li><p>Scalability: designed to work with large volumes of documents, tested with several million documents.</p></li><li><p>Performance: written in C/C++ for high performance.</p></li><li><p>Configurable: all the parameters for crawling and indexing can be configured via an XML file.</p></li><li><p>Analysis: includes several tools for analyzing, extracting statistics, and generating reports on sub-sets of the web, e.g.: the web of a country or a large intranet.</p></li><li><p>Open-source: code is freely available.</p></li></ul></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="installation"></a>Chapter 2. Installation</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#installation-download">Downloading</a></span></dt><dt><span class="section"><a href="#installation-requirements">Requirements</a></span></dt><dt><span class="section"><a href="#installation-overview">Installing</a></span></dt><dt><span class="section"><a href="#installation-cvs">If you have access to the CVS</a></span></dt><dt><span class="section"><a href="#installation-tunning">Kernel tunning</a></span></dt></dl></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="installation-download"></a>Downloading</h2></div></div></div><p>The home page of WIRE is <a class="ulink" href="http://www.cwr.cl/projects/WIRE" target="_top">http://www.cwr.cl/projects/WIRE/</a></p><p>The last version can be downloaded from <a class="ulink" href="http://www.cwr.cl/projects/WIRE/releases/" target="_top">http://www.cwr.cl/projects/WIRE/releases/</a></p><p>If you use WIRE, it is advisable to  <a class="ulink" href="http://groups.yahoo.com/group/wire-crawler/join" target="_top">join the wire-crawler@groups.yahoo.com mailing list</a> to receive announcements of new releases.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="installation-requirements"></a>Requirements</h2></div></div></div><p>The following packages are required to install WIRE:</p><div class="itemizedlist"><ul type="disc"><li><p><a class="ulink" href="http://www.chiark.greenend.org.uk/%7Eian/adns/" target="_top">adns</a> - Asynchronous DNS resolver (note that as of May 2006 the resolver still does not handle DNS chains, a non-standar behavior implemented by some Websites like microsoft.com and others from akamai; a <a class="ulink" href="http://hugo.vulcano.cl/development/adns-cc" target="_top">patched version of adns</a> is available).</p></li><li><p><a class="ulink" href="http://www.libxml.org/" target="_top">xml2</a> - XML library, including xml and XPath parsers, VERSION 2.6 or newer</p></li><li><p><a class="ulink" href="http://swish-e.org/" target="_top">swish-e</a> - The search engine uses swish-e.</p></li></ul></div><p>The following packages are suggested for best results:</p><div class="itemizedlist"><ul type="disc"><li><p>Also, LaTeX (with fullpage.sty, included in tetex-extras in some distros) and <a class="ulink" href="http://www.gnuplot.info/" target="_top">gnuplot</a> are required to generate the reports.</p><p><a class="ulink" href="http://cr.yp.to/djbdns.html" target="_top">djbdns</a> - Useful for setting a local DNS cache</p><p>docbook-xsl - Required for generating locally the documentation</p></li></ul></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>Before running the application, you must set the environment variable <code class="envar">WIRE_CONF</code>.</p></div><p>To get optimal results, i.e., to be able to use the crawler with a high degree of parallelization, see <a class="link" href="#installation-tunning" title="Kernel tunning">kernel tunning</a>.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="installation-overview"></a>Installing</h2></div></div></div><p>To install, unpack the distribution, cd to the created directory, and execute:</p><pre class="screen">
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>./configure</strong></span> </code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>make</strong></span> </code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>make install</strong></span> </code></strong>
      </pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="installation-cvs"></a>If you have access to the CVS</h2></div></div></div><p>If you checkout the distribution from the CVS, you need to do the following in the WIRE directory:</p><pre class="screen">
        <code class="prompt">%</code><strong class="userinput"><code>aclocal</code></strong>
        <code class="prompt">%</code><strong class="userinput"><code>autoheader</code></strong>
        <code class="prompt">%</code><strong class="userinput"><code>automake -a</code></strong>
        <code class="prompt">%</code><strong class="userinput"><code>autoconf</code></strong>
      </pre><p>Before executing <span class="command"><strong>./configure</strong></span>; this is to be able to use the distribution with different versions of autoconf.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="installation-tunning"></a>Kernel tunning</h2></div></div></div><p>The kernel has to be tuned for optimal performance. This can be easily done using the /proc filesystem in Linux (tested in Kernel 2.4).</p><p>Log in as root and issue the following commands:</p><pre class="screen">
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>echo</strong></span> 32768 &gt; /proc/sys/fs/file-max</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>echo</strong></span> 131072 &gt; /proc/sys/fs/inode-max</code></strong>
      </pre><p>This sets the maximum number of open files and inodes.</p><p>Sometimes you must also set these limits as per-user, in Linux, edit the <code class="filename">/etc/security/limits.conf</code> file, adding these lines:</p><pre class="screen">
		*	soft	nofile	32000
		*	hard	nofile	32000
	  </pre><p>You must make sure that the <code class="filename">/etc/pam.d/login</code> file includes the limits file:</p><pre class="screen">
		session	required	pam_limits.so
	  </pre><p>Note that changing user limits requires the user to logout/login to apply changes.</p><p>For compile-time directives regarding limits, see the section on <a class="link" href="#repository-storage-lfs" title="Large file support">large file support</a>.</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="configuration"></a>Chapter 3. Configuration</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#configuration-overview">Overview</a></span></dt><dt><span class="section"><a href="#configuration-example">Sample configuration</a></span></dt></dl></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration-overview"></a>Overview</h2></div></div></div><p>
			The configuration file is an XML file. The environment variable
			<code class="envar">WIRE_CONF</code> must point to this configuration file. We provide a sample,
            commented configuration file that includes several of the defaults we use. However,
            there are some parameters you must set before starting the collection.
		</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration-example"></a>Sample configuration</h2></div></div></div><p>
			The exact structure varies between versions. See the file
			<a class="ulink" href="sample.conf" target="_top">sample.conf</a>
			for an up-to-date example. On this documentation,
			<em class="replaceable"><code>config/x/y</code></em> refers to a variable in the configuration file.
		</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="repository"></a>Chapter 4. Data Format and API for the Repository</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#repository-overview">Overview</a></span></dt><dt><span class="section"><a href="#repository-storage">Text and HTML storage (text/)</a></span></dt><dd><dl><dt><span class="section"><a href="#repository-storage-lfs">Large file support</a></span></dt><dt><span class="section"><a href="#repository-storage-api">API</a></span></dt></dl></dd><dt><span class="section"><a href="#repository-metaidx">Metadata index (metaidx/)</a></span></dt><dd><dl><dt><span class="section"><a href="#repository-metaidx-api">API</a></span></dt></dl></dd><dt><span class="section"><a href="#repository-urlidx">URL index (urlidx/)</a></span></dt><dd><dl><dt><span class="section"><a href="#repository-urlidx-api">API</a></span></dt></dl></dd><dt><span class="section"><a href="#repository-linkidx">Link index (link/)</a></span></dt><dt><span class="section"><a href="#repository-harvestidx">Harvest index (harvest/)</a></span></dt></dl></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repository-overview"></a>Overview</h2></div></div></div><p>The data format of the collection or repository is designed to scale to several million documents. It is composed of several directories. Each directory is pointed to by a configuration variable.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repository-storage"></a>Text and HTML storage (text/)</h2></div></div></div><p>The storage subsystem is a large file and some data structures designed to store
		variable-length records containing the downloaded pages and to detect duplicates.</p><p>It is implemented using a free-space list with first-fit allocation,
		and a hash table for duplicate detections. It never stores an exact
		duplicate of something you have stored before.</p><p>There is an internal limit of <code class="varname">MAX_DOC_LEN</code> bytes of storage for each document stored, this is set in the source in the file <code class="code">lib/const.h</code></p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>The system must provide large file support, to be able to create files over 4GB long, if the collection is large, enabling large-file support is explained in the next section. Most modern filesystems support these types of files.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="repository-storage-lfs"></a>Large file support</h3></div></div></div><p>All of your source files that needs to be linked with this library must use <code class="filename">config.h</code>, and those
		who need <code class="varname">O_LARGEFILE</code> must
		include <code class="filename">features.h</code>.</p><p>Otherwise, the compiler will complain because <span class="type">off_t</span> is usually <span class="type">long</span>, but it should be <span class="type">long long</span> with
		large file support; the error reported by the linker is usually
		very cryptic (related to something missing in the STL, for instance).</p><p>
		To check if everything was compiled OK, use:</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>nm</strong></span> --demangle libwire.a | grep storage_read</code></strong>
        </pre><p>There should be at least one argument of type <span class="type">long long</span>, this indicates that <span class="type">off_t</span> is correct.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="repository-storage-api"></a>API</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-storage-api-opening"></a>Opening</h4></div></div></div><p>To open a storage, use:</p><pre class="programlisting">storage_t *storage = storage_open( char *dirname );</pre><p><code class="varname">dirname</code> must be a valid directory, writable by the effective uid. The program will create the required files if necessary. To close, use:</p><pre class="programlisting">storage_close( storage_t *storage );</pre><p>This should be called in the cleanup handler of your application.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-storage-api-storing"></a>Storing documents</h4></div></div></div><p>To save a document in the storage, you need content and a docid for that document:</p><pre class="programlisting">
char *buf = ".....";  /* Document contents */
docid_t docid = xxxx; /* Usually obtained via urlidx */
off_t len = (off_t)strlen( buf );
</pre><p>Now you can store the document, checking for the results of the function.</p><pre class="programlisting">
doc_hash_t doc_hash;
docid_t duplicate_of;

storage_status_t rc;
rc = storage_write( storage, docid, buf, len, &amp;(doc_hash), &amp;(duplicate_of) );
if( rc == STORAGE_UNCHANGED ) {

/* Document unchanged, already stored with the same content and docid */

} else if( rc == STORAGE_DUPLICATE ) {

/* Document is duplicate, not stored
variable duplicate_of contains the docid of the older
document with the same content */

} else if( rc == STORAGE_OK ) {

/* Document stored */

}
</pre><p>The return values of storage_write are the following:</p><p><code class="varname">STORAGE_UNCHANGED</code>: The document already exists with the same docid and content, so it has not changed.</p><p><code class="varname">STORAGE_DUPLICATE</code>: There is another document (its docid will be returned in duplicate_of) with the same content.</p><p><code class="varname">STORAGE_OK</code>: The document was stored. Maybe it already existed with the same docid, but the content has changed, or it is a new document.</p><p>In all cases, the variable doc_hash will contain a hash function of the document content.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-storage-api-reading"></a>Reading documents</h4></div></div></div><p>To read a stored document, you need first the docid of the document:</p><pre class="programlisting">
docid_t docid = xxxx;
</pre><p>Now, you must allocate memory and retrieve the document.</p><pre class="programlisting">
char *buf = (char *)malloc(sizeof(char)*MAX_DOC_LEN);
off_t len;
storage_status_t rc;

rc = storage_read( storage, docid, buf, &amp;(len) );
if( rc == STORAGE_NOT_FOUND ) {
/* Document inexistent */
} else if( rc == STORAGE_OK ) {
/* Document OK */
}
</pre><p>The return values of storage_read are the following:</p><p><code class="varname">STORAGE_NOT_FOUND</code>: The requested docid is not stored.</p><p><code class="varname">STORAGE_DUPLICATE</code>: The requested docid was retrieved to buf.The content length is in length</p><p>To check if a document exists without loading it to memory, use:</p><pre class="programlisting">
docid_t docid = xxxx;
bool rc = storage_exists( storage, docid );
</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-storage-api-deleting"></a>Deleting a document</h4></div></div></div><p>To remove a document from the storage, use:</p><pre class="programlisting">
docid_t docid = xxxx;
storage_delete( storage, docid );
</pre><p>Notice that the storage subsystem does not save a list of all the duplicates of a document, so if you delete the main document, the duplicates pointer of the others will point to an inexistent document.</p></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repository-metaidx"></a>Metadata index (metaidx/)</h2></div></div></div><p>A metadata index is a composed of two files of fixed-size records, one for the metadata about documents, and one for the metadata about sites.</p><p>Both data types: <span class="type">doc_t</span> and <span class="type">site_t</span> are defined in <code class="filename">metaidx.h</code></p><p>The first record in each of these files is special. The second record is for the document with docid=1, the third record for docid=2, etc. This is useful because docid=0 is forbidden, and information about document k is located at offset sizeof(doc_t) * k.</p><p>The URL of a document, and the hostname of a site, are not stored in the metadata index, because they are variable-sized records. They are stored in the url index.</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="repository-metaidx-api"></a>API</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-metaidx-api-opening"></a>Opening and closing</h4></div></div></div><p>A metadata index is opened with the command:</p><pre class="programlisting">metaidx_open( char *directory_name, bool readonly );</pre><p>The directory_name must already exist as a directory, and must be writable by the effective uid. The library checks for the files, and creates them if necessary. If readonly is true, you cannot write data to the metaidx.</p><p>To close the metadata index, use</p><pre class="programlisting">metaidx_close( metaidx );</pre><p>Notice that this function should be called from the cleanup handler of your application.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-metaidx-api-retrieving"></a>Retrieving information</h4></div></div></div><p>To retrieve from the metadata file, the programmer must supply the required memory for the object, allocating a <span class="type">doc_t</span> variable and must set the field <code class="varname">doc.docid</code>.</p><pre class="programlisting">
doc_t doc;
doc.docid = 42;
metaidx_doc_retrieve( metaidx, &amp;(doc) );
</pre><p>A status code will be returned; usually all errors generated by this library are fatal.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-metaidx-api-storing"></a>Storing information</h4></div></div></div><p>To store in the metadata file, the programmer must set all the data.</p><pre class="programlisting">
doc_t doc;
doc.docid = 42;
doc.xxxx = yyyy;
metaidx_doc_store( metaidx, &amp;(doc) );
</pre><p>And to store a site:</p><pre class="programlisting">
site_t site;
site.siteid = 38;
site.xxxx = yyyy;
metaidx_site_store( metaidx, &amp;(site) );
</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-metaidx-api-counting"></a>Counting</h4></div></div></div><p>To check how many documents are stored, use</p><pre class="programlisting">
metaidx_t metaidx = metaidx_open( dirname );
docid_t ndocs = metaidx-&gt;count_doc;
siteid_t nsites = metaidx-&gt;count_site;
</pre></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repository-urlidx"></a>URL index (urlidx/)</h2></div></div></div><p>This module provides access to an index of site-names (such as: www.yahoo.com), and paths (such as: customers/help/index.html ). The module is optimized to provide fast responses for these queries:</p><p>Site name to/from siteid: Converts a string, representing a sitename, into a siteid and viceversa.</p><p>Site-id + Path to/from docid: Converts a string, representing a path into a site identified by siteid, into a docid and viceversa.</p><p>The functions are implemented using on-disk hash tables, and in the case of site names, this hash table is loaded into memory for faster access.</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="repository-urlidx-api"></a>API</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-urlidx-api-opening"></a>Opening and closing</h4></div></div></div><p>To open a url index, provide the name of a directory to store the hash tables:</p><pre class="programlisting">urlidx_t *urlidx = urlidx_open( char *dirname, bool readonly );</pre><p>The directory must exist, and be writable by the current uid. The urlidx will create the required files.To close the url index, use</p><pre class="programlisting">urlidx_close( urlidx );</pre><p>This function should be in the cleanup handler of your application. Note that many operations are done in memory, so if you do not close the urlidx, some changes might be lost.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-urlidx-api-queryingsite"></a>Querying for site names</h4></div></div></div><p>To get the siteid for a site name, use:</p><pre class="programlisting">
siteid_t siteid;
urlidx_status_t rc;
rc = urlidx_resolve_site( urlidx, "www.yahoo.com", &amp;(siteid) );
if( rc == URLIDX_EXISTENT ) {

/* The siteid existed and was returned */

} else if( rc == URLIDX_CREATED_SITE ) {

/* A new siteid was allocated and returned */

}
</pre><p>If the site already exists, this will return the corresponding siteid, otherwise, a new siteid will be allocated and returned in the variable siteid.</p><p>To make the reverse query, i.e.: asking for the site name of a siteid, use:</p><pre class="programlisting">
siteid_t siteid = xxxx;
char sitename[MAX_STR_LEN];
urlidx_site_by_siteid( urlidx, siteid, sitename );
</pre><p>Notice that the protocol part of the url (http://) is not saved here. The protocol is considered as a metadata in this framework. Don't add the protocol to the sitename.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="repository-urlidx-api-queryingpath"></a>Querying for paths</h4></div></div></div><p>To query for a path or file inside a website, you need to get first the siteid of the site in which the file or directory is located.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>To save space, path names never contain the leading slash. "/sports/tennis.html" is wrong, but "sports/tennis.html" is right.</p></div><p>Suppose you have the following url "http://www.yahoo.com/news/science.cgi". The first step is to query for the siteid:</p><pre class="programlisting">
siteid_t siteid;
urlidx_resolve_site( urlidx, "www.yahoo.com", &amp;(siteid) );
</pre><p>Now, we can query for the path:</p><pre class="programlisting">
urlidx_status_t rc;
docid_t docid;
rc = urlidx_resolve_path( urlidx, siteid, "news/science.cgi", &amp;(docid) );
if( rc == URLIDX_EXISTENT ) {

/* The document existed */

} else {

/* A new docid was allocated and returned */

}
</pre><p>If the path didn't exists, a new docid will be allocated. To make the reverse query (asking for the path of a docid), you don't need to know the siteid:</p><pre class="programlisting">
char path[MAX_STR_LEN];
docid_t docid = xxxx;
urlidx_path_by_docid( urlidx, docid, path );
</pre></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repository-linkidx"></a>Link index (link/)</h2></div></div></div><p>Links found but not added to the collection (e.g. to multimedia files, etc.) are saved as text files.</p><p>Adjacency lists for the graph</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repository-harvestidx"></a>Harvest index (harvest/)</h2></div></div></div><p>One directory per harvest round.</p><p>Fixed-size records for metadata</p><p>Lists of strings for site names and paths</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="bot"></a>Chapter 5. Web crawler</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#bot-reset">wire-bot-reset</a></span></dt><dt><span class="section"><a href="#bot-seeder">wire-bot-seeder</a></span></dt><dt><span class="section"><a href="#bot-manager">wire-bot-manager</a></span></dt><dd><dl><dt><span class="section"><a href="#bot-manager-score">Score function</a></span></dt></dl></dd><dt><span class="section"><a href="#bot-harvester">wire-bot-harvester</a></span></dt><dt><span class="section"><a href="#bot-gatherer">wire-bot-gatherer</a></span></dt><dt><span class="section"><a href="#bot-run">wire-bot-run</a></span></dt><dt><span class="section"><a href="#bot-howto">Step-by-step instructions for running the crawler</a></span></dt><dd><dl><dt><span class="section"><a href="#bot-howto-1">1. Create the directory for the collection</a></span></dt><dt><span class="section"><a href="#bot-howto-2">2. Copy the sample configuration file into the directory</a></span></dt><dt><span class="section"><a href="#bot-howto-3">3. Set the environment variable <code class="envar">WIRE_CONF</code></a></span></dt><dt><span class="section"><a href="#bot-howto-4">4. Edit the configuration file, mandatory parameters</a></span></dt><dt><span class="section"><a href="#bot-howto-5">5. Create a blank collection</a></span></dt><dt><span class="section"><a href="#bot-howto-6">6. Add starting URLS</a></span></dt><dt><span class="section"><a href="#bot-howto-7">7. Run TWO test crawler cycles</a></span></dt><dt><span class="section"><a href="#bot-howto-8">8. Run several crawler cycles</a></span></dt></dl></dd><dt><span class="section"><a href="#run-stop">Stopping the crawler</a></span></dt></dl></div><p>The web crawler is composed of several programs.</p><p>Normal operation of the web crawler involves repeatedly running the cycle "seeder-manager-harvester-gatherer-seeder" ... many times.</p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="bot-reset"></a>wire-bot-reset</h2></div></div></div><p>Clears the repository, creates empty data structures and prepares everything for a new crawling. As some of the data structures require the disk space to be allocated from the beginning, this will take some time depending on your settings for <code class="varname">maxdoc</code> and <code class="varname">maxsite</code> in the configuration file.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="bot-seeder"></a>wire-bot-seeder</h2></div></div></div><p>Receives URLs from the gatherer (or from the initial URLs) and adds documents for them to the repository. This is used both to give the crawler the initial set of URLs and to parse the URLs that are extracted by the gatherer program from the downloaded pages.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="bot-manager"></a>wire-bot-manager</h2></div></div></div><p>Creates batches of documents for the harvester to download.</p><p>This programs sorts the documents from the collection by their "scores" and creates a batch of documents for the harvester. The scores are given by a combination of factors described in the configuration file.</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-manager-score"></a>Score function</h3></div></div></div><p>The score of each page is calculated as defined in the configuration file; this function currently includes pagerank, depth, dynamic/static pages.</p><p>The manager tries to determine for each document which is the probability of that document being outdated. If this probability is, say, 0.7, then its current score is defined to be 0.7 x score. Its future score will be 1 x score.</p><p>The set of pages that have the higher difference between future score and current score will be selected for the next harvester round.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="bot-harvester"></a>wire-bot-harvester</h2></div></div></div><p>This program downloads the documents from the Web. The program works in its own directory with its own data structures, and can be stopped at any time.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>If for any reason the harvester fails, you must cancel the current batch using <span class="command"><strong>wire-bot-manager --cancel</strong></span>, and then re-generate the current batch using <span class="command"><strong>wire-bot-manager</strong></span>.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="bot-gatherer"></a>wire-bot-gatherer</h2></div></div></div><p>Parses downloaded documents and extract URLs. This takes the pages downloaded by the harvester from its directory, and merges those pages into the main collection.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="bot-run"></a>wire-bot-run</h2></div></div></div><p>Run several crawler cycles of the form "seeder-manager-harvester-gatherer"</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="bot-howto"></a>Step-by-step instructions for running the crawler</h2></div></div></div><p>The following assumes that you have downloaded and installed the WIRE program files.</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-1"></a>1. Create the directory for the collection</h3></div></div></div><p>Decide on which directory is the collection going to be, in this example, we will assume the <em class="replaceable"><code>/opt/wiredata</code></em> directory is used, and then create that directory:</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>mkdir</strong></span> /opt/wiredata</code></strong>
        </pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-2"></a>2. Copy the sample configuration file into the directory</h3></div></div></div><p>The purpose of this is to be able to use different configuration files in different collections.</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>cp</strong></span> /usr/local/share/WIRE/sample.conf /opt/wiredata/wire.conf</code></strong>
        </pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-3"></a>3. Set the environment variable <code class="envar">WIRE_CONF</code></h3></div></div></div><p>Under tcsh:</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>setenv</strong></span> WIRE_CONF /opt/wiredata/wire.conf</code></strong>
        </pre><p>Under bash/sh:</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code>WIRE_CONF=/opt/wiredata/wire.conf; <span class="command"><strong>export</strong></span> WIRE_CONF</code></strong>
        </pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-4"></a>4. Edit the configuration file, mandatory parameters</h3></div></div></div><p>Edit the file <code class="filename">/opt/wiredata/wire.conf</code>. This is an XML file containing the configuration.</p><p>Set the base directory of the collection:</p><p><code class="varname">config/collection/base</code>=<em class="replaceable"><code>/opt/wiredata</code></em></p><p>And set the top level domain:</p><p><code class="varname">config/seeder/accept/domain-suffixes</code>=Set to the top level domain(s) you want to crawl</p><p>Now set the limits for maximum number of documents and sites:</p><p><code class="varname">config/collection/maxdoc</code>: Set higher than the estimated maximum number of documents in the collection</p><p><code class="varname">config/collection/maxsite</code>: Set higher than the estimated maximum number of sites in the collection</p><p>The later are usually set based on the number of domains, i.e.: for a domain with 50,000 domains, we usually set maxsite to 150,000 and maxdoc to 15,000,000.</p><p>The last mandatory parameter is the IP address of the DNS resolvers. See the file <code class="filename">/etc/resolv.conf</code> in your machine to set this configuration. Note that it is better to have several DNS resolvers, otherwise there will be many DNS lookup errors as a single DNS server will not be able to handle all the requests by WIRE. However, WIRE retries several times for each site name, so eventually even with a single DNS server many site names will be found.</p><p><code class="varname">config/harvester/resolvconf</code>: Add one "nameserver" line per each name server</p><p>The other values are set to rather conservative defaults; see the sample.conf file for more details.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-5"></a>5. Create a blank collection</h3></div></div></div><p>This command will cleanup an existing collection, and prepare the directories for a new collection.</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-reset</strong></span></code></strong>
        </pre><p>It may take a while, depending on your settings for maxdoc and maxsite.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-6"></a>6. Add starting URLS</h3></div></div></div><p>You need a file with a set of starting URLs. This file contains absolute URLs, one URL per line; it is customary to leave this file inside the collection directory. Copy it, for example, to <code class="filename">/opt/wiredata/start_urls.txt</code>.</p><p>Example:</p><pre class="screen">
http://www.site1.zzz/
http://www.site2.zzz/
http://www.site2.zzz/inner/urls/
http://www.site3.zzz/
</pre><p>Note that the 'http' must be included in the start url file. If you have IP addresses for some of the Web sites, include them like this:</p><pre class="screen">
http://www.site1.zzz/ IP=145.123.12.9
http://www.site2.zzz/ IP=68.19.145.4
...
</pre><p>To load it into the repository, use:</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-seeder</strong></span> --start <em class="replaceable"><code>/opt/wiredata/start_urls.txt</code></em></code></strong>
        </pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-7"></a>7. Run TWO test crawler cycles</h3></div></div></div><p>Run a test crawler cycle to see how is everything going. The first crawler cycle is special as it will only download robots.txt files, and will resolve the IP addresses of the seed URLS. Use the following command line to run two test cycles.</p><pre class="screen">
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-manager</strong></span></code></strong>
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-harvester</strong></span></code></strong>
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-gatherer</strong></span></code></strong>
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-seeder</strong></span></code></strong>
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-manager</strong></span></code></strong>
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-harvester</strong></span></code></strong>
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-gatherer</strong></span></code></strong>
			<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-seeder</strong></span></code></strong>
		</pre><p>If the four programs succeed in the two rounds, then it is most likely than the configuration is correct. Pay special attention to the output of the second <span class="command"><strong>wire-bot-harvester</strong></span>, to see if it is able to contact the given websites. If the harvester fails, you must run <span class="command"><strong>wire-bot-manager --cancel</strong></span> to cancel the current cycle before continuing.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="bot-howto-8"></a>8. Run several crawler cycles</h3></div></div></div><p>Use the included program wire-bot-run to run several cycles of the crawler; consider that in each cycle at most <code class="varname">WIRE_CONF:config/manager/batch/size</code> documents will be downloaded; so if the batch size is 100,000 documents, 50 cycles will download at most 5 million pages; normally it will be about half of that value because of page errors, saturated servers, etc.</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>nohup</strong></span> wire-bot-run 50 &gt;&amp; /opt/wiredata/run.log &amp;</code></strong>
        </pre><p>If you have sysadmin privileges, you can run the crawler at maximum priority using "nice". The following line locates the process-id of wire-bot-run and changes it to the highest priority (you must run this command as root):</p><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>nice</strong></span> -19 `ps -o "%p" --no-headers -C wire-bot-run`</code></strong>
        </pre></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="run-stop"></a>Stopping the crawler</h2></div></div></div><p>To stop the crawler, you have to first verify in which stage the crawler is running. To do so, do <span class="command"><strong>ps -efa | grep wire</strong></span></p><p>If the current process is the <span class="command"><strong>wire-bot-harvester</strong></span> you can safely stop it using:</p><pre class="screen">
<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>killall wire-bot-run</strong></span></code></strong>
<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>killall wire-bot-harvester</strong></span></code></strong>
<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-bot-manager --cancel</strong></span></code></strong>
	</pre><p>The last command is absolutely necessary, otherwise, a partial harvest round will be left.</p><p>If the current process is not the harvester, you have to wait until it is, and then stop the crawler. It is not a good idea to interrupt neither the gatherer nor the seeder.</p><p>If the machine hangs, or power fails, before running <span class="command"><strong>wire-bot-run</strong></span> you have to run <span class="command"><strong>wire-bot-manager --cancel</strong></span>.</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="info"></a>Chapter 6. Statistics and reports</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#info-shell">Interactive querying</a></span></dt><dd><dl><dt><span class="section"><a href="#info-shell-main">Main commands</a></span></dt><dt><span class="section"><a href="#info-shell-doc">Information about a document</a></span></dt><dt><span class="section"><a href="#info-shell-site">Information about a site</a></span></dt><dt><span class="section"><a href="#info-shell-harvester">Information about a harvester</a></span></dt><dt><span class="section"><a href="#info-shell-repository">Information about the collection</a></span></dt></dl></dd><dt><span class="section"><a href="#info-extract">Extract raw data</a></span></dt><dt><span class="section"><a href="#info-analysis">Analyzing data to generate statistics</a></span></dt><dd><dl><dt><span class="section"><a href="#info-analysis-link">Link analysis in the graph of pages</a></span></dt><dt><span class="section"><a href="#info-analysis-sitelink">Link analysis in the graph of websites</a></span></dt><dt><span class="section"><a href="#info-analysis-stats">Generating statistics</a></span></dt></dl></dd><dt><span class="section"><a href="#info-report">Generating reports</a></span></dt><dd><dl><dt><span class="section"><a href="#info-report-conf">Configuring the reports</a></span></dt><dt><span class="section"><a href="#info-report-procedure">Procedure for generating reports</a></span></dt><dt><span class="section"><a href="#info-report-doc">Report about documents</a></span></dt><dt><span class="section"><a href="#info-report-site">Report about sites</a></span></dt><dt><span class="section"><a href="#info-report-link">Report about the graph of links between sites</a></span></dt><dt><span class="section"><a href="#info-report-harvest">Report about the harvest batches</a></span></dt><dt><span class="section"><a href="#info-report-lang">Report about document languages</a></span></dt><dt><span class="section"><a href="#info-report-extension">Report about the link extensions</a></span></dt></dl></dd></dl></div><p>These tools are designed to extract data from the repository to text files and to generate reports.</p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="info-shell"></a>Interactive querying</h2></div></div></div><p><span class="command"><strong>wire-info-shell</strong></span> is an interactive program for querying the collection. Type <span class="command"><strong>help</strong></span> at the prompt to view the list of commands, or <span class="command"><strong>quit</strong></span> to exit the session.</p><p>All the commands write their output to the standard output. This is the list of commands:</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-shell-main"></a>Main commands</h3></div></div></div><p><span class="command"><strong>help</strong></span>: Prints a help message with all the commands.</p><p><span class="command"><strong>quit</strong></span>:	Exists the program</p><p><span class="command"><strong>summary</strong></span>: Shows summary information about the crawler</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-shell-doc"></a>Information about a document</h3></div></div></div><p><span class="command"><strong>doc</strong></span><em class="replaceable"><code>docid</code></em>: Prints all the metadata of the specified document-id. Prints the url of the specified document-id.</p><p><span class="command"><strong>read</strong></span><em class="replaceable"><code>docid</code></em>: Prints the content of a document.</p><p><span class="command"><strong>links</strong></span><em class="replaceable"><code>docid</code></em>: Prints all the links of a document.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-shell-site"></a>Information about a site</h3></div></div></div><p><span class="command"><strong>site</strong></span><em class="replaceable"><code>siteid</code></em>: Prints all the metadata of the specified site-id.  Prints the hostname of the specified site-id.</p><p><span class="command"><strong>site</strong></span><em class="replaceable"><code>sitename</code></em>: Searches for the site-id of the specified hostname.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-shell-harvester"></a>Information about a harvester</h3></div></div></div><p><span class="command"><strong>harvester</strong></span><em class="replaceable"><code>harvestid</code></em>: Shows data about a harvester batch.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-shell-repository"></a>Information about the collection</h3></div></div></div><p><span class="command"><strong>urlidx</strong></span>: Check the status of url index</p><p><span class="command"><strong>metaidx</strong></span>: Check the status of the metadata index.  This is useful to get the number of known pages.</p><p><span class="command"><strong>linkidx</strong></span>: Check the status of the link index</p><p><span class="command"><strong>storage</strong></span>: Check the status of the storage</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="info-extract"></a>Extract raw data</h2></div></div></div><p><span class="command"><strong>wire-info-extract</strong></span> is a tool for extracting readable copies of the contents of the repository, to make analysis with external programs. The data format for the output of <span class="command"><strong>wire-info-extract</strong></span> is comma-separated-values, i.e.: one record per line, and multiple fields separated by commas. The first record on each file contain the name of the fields.</p><p>See:</p><pre class="screen">
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-extract</strong></span> -h</code></strong>
	  </pre><p>For a list of available options.</p>.
	  <p>A very useful command is:</p><pre class="screen">
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-extract</strong></span> --seeds</code></strong>
	  </pre><p>As it can be used to generate the list of starting URLs for the next crawl, including all pages with at least one page downloaded OK, and including their IP addresses. Note that you must run <span class="command"><strong>wire-info-analysis --site-statistics</strong></span> first, to count the number of downloaded pages per site.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="info-analysis"></a>Analyzing data to generate statistics</h2></div></div></div><p><span class="command"><strong>wire-info-analysis</strong></span> is a program for generating statistics, all the statistics will be generated into the <code class="filename">analysis/</code> directory.</p><p>For help on the different statistics available, use:</p><pre class="screen">
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> -h</code></strong>
      </pre><p>All of the analysis phases are listed in order. This program works in several phases or steps. On each step, a command line switch must be entered. The full sequence must be entered in this order:</p><pre class="screen">
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --pagerank</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --hits</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --sitelink-analysis</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --doc-statistics</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --site-statistics</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --extension-statistics</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --harvest-statistics</code></strong>
        <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --lang-statistics</code></strong>
      </pre><p>Note that the link analysis phases must me done first, as they generate data that is used by the other phases.</p><div class="warning" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Warning</h3><p>Some of the statistics may take a long time, specially those related with link analysis. DO NOT USE --link-analysis unless you have enough memory for the task: it is better to run each link graph analysis in a separate process (see below).</p></div><p>Link analysis involves an iterative computation until a target average error is reached. This process can be controlled using <code class="varname">WIRE_CONF:config/manager/score/*/max-error</code>.</p><p>Each analysis phase has a command line switch. The command line switches are explained below:</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-analysis-link"></a>Link analysis in the graph of pages</h3></div></div></div><p><span class="command"><strong>--link-analysis</strong></span>: This is equivalent to <span class="command"><strong>--pagerank --hits --wlrank</strong></span>.DO NOT USE this form unless you have enough memory for the complete task, it is better to run the following tasks in separate process.</p><p><span class="command"><strong>--pagerank</strong></span>: Generates the Pagerank of the pages. The calculation requires about 25Mb of memory for each million documents. <code class="varname">doc.pagerank</code> is written to the metadata</p><p><span class="command"><strong>--wlrank</strong></span>: Generates the weighted link rank of pages. This is pagerank weighted by the tag and position of the link in the page. The calculation requires about 25Mb of memory for each million documents. <code class="varname">doc.wlrank</code> is written to the metadata</p><p><span class="command"><strong>--hits</strong></span>: Generates the static Hubs and Authorities score of pages. This uses Bharat's heuristic of discarding internal links, using only links between pages at different websites. The calculation requires about 50Mb of memory for each million documents. <code class="varname">doc.hub</code> and <code class="varname">doc.authrank</code> are written to the metadata</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-analysis-sitelink"></a>Link analysis in the graph of websites</h3></div></div></div><p><span class="command"><strong>--sitelink-analysis</strong></span>: This is equivalent to --sitelink-generate --sitelink-component --sitelink-siterank</p><p><span class="command"><strong>--sitelink-generate</strong></span>: Generates a graph with links, in which all the links to and from pages on the same site are collapsed to a single link. This only generates the graph. Files with links are written to: <code class="filename">sitelink/</code></p><p><span class="command"><strong>--sitelink-components</strong></span>: Site components are generated using Broder's graphs components based on the biggest strongly-connected-component. The sites considered are only those with at least one page downloaded. Component and link-statistics of each site are written to metadata. Statistics are written to <code class="filename">analysis/sitelink/</code></p><p><span class="command"><strong>--sitelink-siterank</strong></span>: An equivalent of pagerank, considering the links between sites, is calculated for each website. <code class="varname">site.siterank</code> is written to the metadata</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-analysis-stats"></a>Generating statistics</h3></div></div></div><p><span class="command"><strong>--doc-statistics</strong></span>: Generates summary statistics and data tables about all the metadata. It will filter the documents by status (all documents, gathered or downloaded documents, and new documents that have not been visited), and by static/dynamic URLs (all documents, static documents, and dynamic-ally generated pages). This generates all the combinations. For general statistics, it is recommended to use the directory doc_gathered_all, as it includes all the pages that have been downloaded, including static and dynamic pages. Files with data are written to <code class="filename">analysis/doc_X_X</code></p><p><span class="command"><strong>--site-statistics</strong></span>: Generates summary statistics and data tables about websites.	Statistics are written to <code class="filename">analysis/site</code></p><p><span class="command"><strong>--extension-statistics</strong></span>: Generates summary statistics about links to domains outside the selected ones, or links to images or multimedia files. Statistics are written to <code class="filename">analysis/extension</code></p><p><span class="command"><strong>--harvest-statistics</strong></span>: Generates statistics about harvest rounds. Statistics are written to <code class="filename">analysis/harvest</code></p><p><span class="command"><strong>--lang-statistics</strong></span>: Generates statistics about languages. Statistics are written to <code class="filename">analysis/lang</code></p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="info-report"></a>Generating reports</h2></div></div></div><p>The analysis package includes programs to generate graphs and data tables. This depends on the following packages:</p><div class="itemizedlist"><ul type="disc"><li><p>perl5</p></li><li><p>XML::LibXML perl module</p></li><li><p>latex (with longtables and fullpage support)</p></li><li><p>gnuplot</p></li></ul></div><p>You must  have those packages installed to generate the reports.</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-conf"></a>Configuring the reports</h3></div></div></div><p>The exact content of the reports in terms of which data tables and graphs will be generated cannot be configured at this time.</p><p>However, it is possible to configure the range for some graphs, as well as the fitting parameters for them. See <code class="varname">WIRE_CONF:config/analysis</code> in sample.conf. Ranges are given in the configuration in the format </p><pre class="screen">[xmin xmax] [ymin ymax]</pre><p>. The default range is </p><pre class="screen">[] []</pre><p> which includes all points, but this default might be in insatisfactory for some graphs. The recommendation is to generate the reports, then view the result, then change the range parameters and re-generate the reports. Note that only the report generation is necessary if you change the range parameters, it is not necessary to repeat the analysis.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-procedure"></a>Procedure for generating reports</h3></div></div></div><p>The  wire-info-analysis program generates multiple .csv (comma-separated values) files in a directory.</p><p>The files are analyzed by a wire-report-* program:</p><div class="orderedlist"><ol type="1"><li><p>For each data table, a <code class="filename">xxx_tab.tex</code> file is generated containing a latex table.</p></li><li><p>For each graph, a <code class="filename">xxx.gnuplot</code> file is generated, containing a script which is later used by gnuplot to generate <code class="filename">xxx.eps</code> an encapsulated postscript file. Additionally, a <code class="filename">xxx_fig.tex</code> file is generated containing the commands for inserting the postscript file into the Latex output.</p></li><li><p>A <code class="filename">report.tex</code> file is generated. LaTeX is then called to generate a .PDF file which is moved to the root of the analysis directory.</p></li></ol></div><p>All these files are generated in a subdirectory of <code class="filename">analysis/</code> under the collection, and they can be modified manually. For instance, the gnuplot scripts can be modified to plot a graph in a different range.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-doc"></a>Report about documents</h3></div></div></div><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --link-analysis</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --doc-statistics</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-report-doc</strong></span>
          </code></strong>
        </pre><p>This report includes statistics about all the documents that were attempted to be downloaded by the crawler. This includes all documents whose status is <code class="varname">STATUS_DOC_GATHERED</code>. The report contains:</p><div class="itemizedlist"><ul type="disc"><li><p>Page depth. Depth=1 is the front page of each site.</p></li><li><p>HTTP code. This is the result code. HTTP codes below 100 are internally used for error conditions in which a successful connection with the HTTP server was not completed.</p></li><li><p>Mime type. Normally, images or multimedia files are not downloaded by the crawler. However, web servers can respond with any mime-type, so this is a list of mime-types of successfully downloaded pages.</p></li><li><p>Document age in months and years. This is the date of last modification minus the date of last visit. To account for differences in the clock of servers, any timestamp in the future, but less than 1 day, is considered as if it were now.</p></li><li><p>Several link score measures, including pagerank, weighted link rank, hub score, authority score, in-degree and out-degree.</p></li><li><p>Raw content length. The size of unparsed data is the number of bytes actually transfered. There is a limit on the number of bytes downloaded, see <code class="varname">WIRE_CONF:config/harvester/maxfilesize</code> for the limit.</p></li><li><p>Content length, the number of bytes kept after removing most of the formatting tags.</p></li></ul></div><p>Additionally, scatter plots will be generated using a sample of the documents. Use <code class="varname">WIRE_CONF:config/analysis/doc/sample-every</code> to control the size of the sample. Note that if the sample size is changed, it is necessary to re-run the <span class="command"><strong>wire-info-analysis</strong></span> program.</p><p>The parameters for the data tables and graphs in this report can be found under <code class="varname">WIRE_CONF:config/analysis/doc</code></p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-site"></a>Report about sites</h3></div></div></div><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --sitelink-analysis</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --site-statistics</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-report-site</strong></span> </code></strong>
        </pre><p>This report includes statistics about websites.</p><p>Note that it is likely that the crawler does not download the entire websites, so some statistics must be considered as bounds on the data. E.g. the content length of the website downloaded by the crawler is a lower bound on the size of the website itself.</p><p>The age of the oldest page of a website is a lower bound on the age of the website itself. e.g.: if the oldest page in a website is 6 months old, then the website exists at least since 6 months ago. The age of the newest page of a website is an upper bound on the update frequency of the website. e.g.: if the newest page in a website is 6 months old, then the website was updated at most 6 months ago.</p><p>The contents of the report include:</p><div class="itemizedlist"><ul type="disc"><li><p>Top websites by number of pages, size, etc.</p></li><li><p>Raw content length. Number of bytes downloaded.</p></li><li><p>Sum of several link analysis statistics.</p></li><li><p>In-degree and out-degree. Number of different sites pointing to/from this site.</p></li><li><p>Siterank. This is like pagerank, but applied to sites.</p></li><li><p>Cumulative distribution and histograms of these statistics.</p></li></ul></div><p>Several scatter plot are also generated relating different statistics. This is done using a sample of one every <code class="varname">WIRE_CONF:config/analysis/site/scatter-plot-every</code> sites. Changing this number requires to execute the wire-info-analysis program again.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-link"></a>Report about the graph of links between sites</h3></div></div></div><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --sitelink-analysis</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-report-sitelink</strong></span> </code></strong>
        </pre><p>This report includes information about the graph of links between sites.</p><p>The contents include:</p><div class="itemizedlist"><ul type="disc"><li><p>Size of components, related to the main strongly connected components.</p></li><li><p>Histogram of strongly-connected component sizes.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-harvest"></a>Report about the harvest batches</h3></div></div></div><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --harvest-analysis</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-report-harvest</strong></span> </code></strong>
        </pre><p>This report includes information about the cumulative score acquired during each harvest round. In all the graphs, time is plotted against some score. This is done to analyze how quickly is the crawler finding important pages in terms of e.g.: link score.</p><p>The contents of this report include:</p><div class="itemizedlist"><ul type="disc"><li><p>All the relevant data about each harvest round.</p></li><li><p>Number of sites included in each round.</p></li><li><p>Number of pages successfully downloaded.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-lang"></a>Report about document languages</h3></div></div></div><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --lang-statistics</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-report-lang</strong></span> </code></strong>
        </pre><p>This report contains statistics about the languages in which documents are written. The heuristic used involves several files containing frequent words on different languages. See <code class="varname">WIRE_CONF:config/analysis/lang</code> for details.</p><p>This report also contains the number of pages in each language found at each page depth.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="info-report-extension"></a>Report about the link extensions</h3></div></div></div><pre class="screen">
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-info-analysis</strong></span> --extension-statistics</code></strong>
          <code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-report-extension</strong></span> </code></strong>
        </pre><p>The links to external domain include images (mostly banners, pointing to .net or .com domains). That's why there is a list that includes only pages. Links to external domains are affected by:</p><div class="orderedlist"><ol type="1"><li><p>How big are external domains, i.e.: .COM is very large and hence very linked</p></li><li><p>There are copies of Open Directory Project data everywhere, so you will always find links to very small countries</p></li><li><p>Popular country-code top level domains that are not used by their meaning as countries; here are some examples:</p></li></ol></div><div class="informaltable"><table border="1"><colgroup><col><col><col></colgroup><tbody><tr><td>Country Code</td><td>Country</td><td>Comercial usage</td></tr><tr><td>.WS</td><td>Samoa</td><td>WebSite</td></tr><tr><td>.TV</td><td>Tuvalu</td><td>TeleVision</td></tr><tr><td>.TO</td><td>Tonga</td><td>Used in "go.to/...", "crawl.to/", etc.</td></tr><tr><td>.TK</td><td>Tokelau</td><td>It sounds cool ?</td></tr><tr><td>.NU</td><td>Niue</td><td>Sounds like "new"</td></tr></tbody></table></div><p>This report contains the following information:</p><div class="itemizedlist"><ul type="disc"><li><p>Most linked top level domains.</p></li><li><p>Most linked top level domains vs size of domains. This is the ratio between the number of links found and the actual size of a domain. If links were uniformly distributed, this should be 1 for each domain, but they are not, and this list attempts to reflect which domains are more popular.</p></li><li><p>Most linked file type extensions, by file type.</p></li></ul></div><p>The file type extensions can be configured using <code class="varname">WIRE_CONF:config/analysis/extension</code></p></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="index"></a>Chapter 7. Indexing and searching</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#index-create">Creating the index</a></span></dt><dt><span class="section"><a href="#index-use">Using the index: normal search</a></span></dt></dl></div><p>WIRE includes a program for creating a swish-e index. This program depends on <a class="ulink" href="http://swish-e.org/" target="_top">SWISH-e</a> to work.</p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="index-create"></a>Creating the index</h2></div></div></div><p>To create the index, use:</p><pre class="screen">
					<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-search-indexer</strong></span> --format swish-e --config --index</code></strong>
				</pre><p>This command first creates a configuration file for SWISH-E (option --config), and then executes SWISH-E (option --index). If you want more control over the indexation parameters, first execute with --config, then edit the configuration file manyall, then execute with --index.</p><p>SWISH-E extracts the data according to the configuration file, and will call <span class="command"><strong>wire-search-feeder</strong></span>, which generates an output that is read by <span class="command"><strong>swish-e</strong></span> and then indexed. The index is generated in the <code class="filename">index</code> subdirectory of the collection. Please refer to the swish-e documentation for instructions on how to search using this index.</p><p>If you want to create a partial index, starting with document number <code class="varname">FIRST_DOCID</code> up to document number <code class="varname">LAST_DOCID</code>, use:</p><pre class="screen">
					<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>wire-search-indexer</strong></span> --format swish-e --from <code class="varname">FIRST_DOCID</code> --to <code class="varname">LAST_DOCID</code></code></strong>
				</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="index-use"></a>Using the index: normal search</h2></div></div></div><p>To execute a query, first create the index using the procedure described above.</p><p>Verify that the index file was created. See the file <code class="filename">index/index.swish-e</code> under your collection's directory.</p><p>To execute a query, use the following (replace "BASE" by the directory of your collection):</p><pre class="screen">
					<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>swish-e</strong></span> -w <code class="varname">query</code> -f <code class="varname">BASE</code>/index/index.swish-e</code></strong>
				</pre><p>Certain properties are saved to the index (e.g.: <code class="filename">swishdescription</code> contains the first bytes of the text), to see the list of properties, use:</p><pre class="screen">
					<code class="prompt">%</code> <strong class="userinput"><code><span class="command"><strong>swish-e</strong></span> -T INDEX_METANAMES -f <code class="varname">BASE</code>/index/index.swish-e</code></strong>
				</pre></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="history"></a>Chapter 8. Version history</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#id2791072">Version 0.22</a></span></dt><dt><span class="section"><a href="#id2791086">Version 0.21</a></span></dt><dt><span class="section"><a href="#id2791139">Version 0.20</a></span></dt><dt><span class="section"><a href="#id2791153">Version 0.19</a></span></dt><dt><span class="section"><a href="#id2791174">Version 0.18</a></span></dt><dt><span class="section"><a href="#id2791190">Version 0.17</a></span></dt><dt><span class="section"><a href="#id2791204">Version 0.16</a></span></dt><dt><span class="section"><a href="#id2791225">Version 0.15</a></span></dt><dt><span class="section"><a href="#id2791246">Version 0.14</a></span></dt><dt><span class="section"><a href="#id2791260">Version 0.13</a></span></dt><dt><span class="section"><a href="#id2791296">Version 0.12</a></span></dt><dt><span class="section"><a href="#id2791329">Version 0.11</a></span></dt><dt><span class="section"><a href="#id2791365">Version 0.10</a></span></dt><dt><span class="section"><a href="#id2791388">Version 0.9b</a></span></dt><dt><span class="section"><a href="#id2791402">Version 0.9</a></span></dt><dt><span class="section"><a href="#id2791425">Version 0.8</a></span></dt></dl></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791072"></a>Version 0.22</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed an issue in a 64bit architecture when the metaidx is more than 4GB; and a rare issue with the gatherer when the parser returns an empty document -- thanks Attila Zséder</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791086"></a>Version 0.21</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed compilation issues in a 64bits architecture, warnings in universalchardet, and a bug in urlidx.cc with respect to relative paths starting in a dynamic URL -- thanks Rhodrigo Meza.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791139"></a>Version 0.20</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed compilation warnings and added a --texts option to the export.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791153"></a>Version 0.19</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed a bug in wire-bot-harvester that affected a tiny portion of web servers (old versions of Netscape Enterprise Server and others). Now all the lines in the HTTP request end in \r\n instead of \n -- thanks Rhodrigo Meza for reporting and help debugging this.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791174"></a>Version 0.18</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed bug in wire-bot-seeder, the outlinks of the last document were not being processed -- thanks Dmitry Ruzanov, Sergei Shebanin and Ivan Sigaev for reporting this.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791190"></a>Version 0.17</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed bug in wire-info-statistics --lang-analysis (perfhash was too small).</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791204"></a>Version 0.16</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed nasty bug in parsing of ROBOTS.TXT files (was saving siteid instead of docid). Thanks to Peter Halacsy and Daniel Varga for reporting the error.</p></li><li><p>Several fixes to prevent buffer overflow errors.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791225"></a>Version 0.15</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Implemented Locality-Sensitive Hashing Sketches using the irudiko library. This was done by Angelo Romano; to use it you can set the parameter <code class="varname">WIRE_CONF:gatherer/use-sketches</code> in the configuration file. This will compute a sketch for each document, which later you can extract using wire-info-extract.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791246"></a>Version 0.14</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed bug in checking site length.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791260"></a>Version 0.13</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed bug that increased CPU usage when there were just a few sites available.</p></li><li><p>Documented IP=x.x.x.x extension.</p></li><li><p>Included link to patched ADNS version in documentation -- thanks to <a class="ulink" href="http://hugo.vulcano.cl/" target="_top">Hugo Salgado</a> and <a class="ulink" href="http://www.rho.cl/" target="_top">Rhodrigo Meza</a>.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791296"></a>Version 0.12</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Exports links to text shows only the links among OK pages and/or redirects.</p></li><li><p>Redirects are considered in PageRank and HITS computation.</p></li><li><p>Fixed error in parsing of robots.txt for cases with multiple user-agent matches.</p></li><li><p>Fixed bug in number format of report library (thanks Bartek!).</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791329"></a>Version 0.11</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Added an option to reduce disk space by keeping only stats on the links to multimedia files. See the sample configuration file: <code class="varname">config/seeder/extensions/</code>. If you are upgrading, we recommend you to copy this part of the sample configuration file to your own.</p></li><li><p>Fixed warnings during the compilation</p></li><li><p>Fixed a nasty bug in the parser, that affected the caption of links under some circumstances</p></li><li><p>Fixed a bug that appeared when the domain suffix was .none, the external links among sites were not saved</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791365"></a>Version 0.10</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Improved performance, reduced memory usage of the seeder and manager program</p></li><li><p>Fixed bug of URL parsing that can cause an unusually high (more than 40%) number of broken links if certain special characters are present in URLs. The error was detected during a crawl of the Polish web, previous crawls should not be affected.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791388"></a>Version 0.9b</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Fixed a bug with the gatherer and collections with more than 50 million documents</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791402"></a>Version 0.9</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Added support for MD5</p></li><li><p>Added configuration settings for UTF-8 conversion</p></li><li><p>Added default configuration settings</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id2791425"></a>Version 0.8</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Introduced the UTF-8 converter</p></li></ul></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="bugs"></a>Chapter 9. Known bugs</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>"Chunked" content-encoding is not supported properly, meaning that some hex numbers (indicating the size of each chunk) will appear in the middle of some pages.</p></li><li><p>Dictionary-based language detection does not work to well, it would be much better to use a bayesian detector. To work around this, you can extract a part of the collection and then run a language detector externally</p></li></ul></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="acknowledgements"></a>Chapter 10. Acknowledgements</h2></div></div></div><p>This project is funded by the Center for Web Research.</p><p>The Center for Web Research (CWR) is possible thanks to the Millennium Program.</p><p>Design:</p><div class="itemizedlist"><ul type="disc"><li><p>Ricardo Baeza-Yates</p></li><li><p>Carlos Castillo</p></li></ul></div><p>Programming:</p><div class="itemizedlist"><ul type="disc"><li><p>Carlos Castillo - Web crawler and repository</p></li><li><p>Emilio Davis - Swish-e integration and weighted link score</p></li><li><p>Felipe Lalanne - Charset detection and several bugfixes</p></li></ul></div></div></div></body></html>
